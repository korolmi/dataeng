{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ввод и вывод в Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура шага\n",
    "\n",
    "**здесь нужно редактирование**\n",
    "\n",
    "* аудио io_intro\n",
    "    * по нему слайд \"источника данных\"\n",
    "* скринкаст 1 про работу с файлами (5-7 минут)\n",
    "* в материалах то, по чему шел скринкаст\n",
    "* скринкаст 2 про работу с базами данных (5-7 минут)\n",
    "* в материалах то, по чему шел скринкаст\n",
    "* дополнительные материалы (фрагменты ноутбуков с комментариями)\n",
    "* задачи для самопроверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Источники данных\n",
    "\n",
    "![](ideal.png)\n",
    "\n",
    "* базы данных (JDBC)\n",
    "* файлы (HDFS)\n",
    "\n",
    "#### С источниками данных можно выполнить операции \n",
    "\n",
    "* читать данные\n",
    "* записывать данные\n",
    "\n",
    "Все это работает максимально эффективно - используется параллелизм кластера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**скринкаст 1**\n",
    "\n",
    "### Работа с файлами\n",
    "\n",
    "Работать с файлами можно локально (т.е. spark рабоает в `local mode`) и в кластере (`yarn mode`), нужны только данные. Мы будем работать с файлами локально, с базами данных - в кластере."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартная \"шапка\" - настройка окружения на работу со spark\n",
    "\n",
    "**Важно** обратите внимание - SPARK_HOME указывает на путь, куда установлен Spark на локальном компютере (поправьте под себя) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/mk/mk_win/projects/SparkEdu/lib/python3.5/site-packages/pyspark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"pyspark-shell\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем сессию, работаем в \"локальном\" режиме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = \"local\"\n",
    "spark = SparkSession.builder.master(master).appName(\"spark_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим наш файл со странами мира, на забываем - первая строка, заголовок (опция `header`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# цепочка обработки при чтении\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"path\", \"data/countries of the world.csv\") \\\n",
    "    .load()\n",
    "# df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажем первые 3 строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+--------------+--------------------------+----------------------------+-------------+----------------------------------+------------------+------------+-----------------+----------+---------+---------+-------+---------+---------+-----------+--------+-------+\n",
      "|     Country|              Region|Population|Area (sq. mi.)|Pop. Density (per sq. mi.)|Coastline (coast/area ratio)|Net migration|Infant mortality (per 1000 births)|GDP ($ per capita)|Literacy (%)|Phones (per 1000)|Arable (%)|Crops (%)|Other (%)|Climate|Birthrate|Deathrate|Agriculture|Industry|Service|\n",
      "+------------+--------------------+----------+--------------+--------------------------+----------------------------+-------------+----------------------------------+------------------+------------+-----------------+----------+---------+---------+-------+---------+---------+-----------+--------+-------+\n",
      "|Afghanistan |ASIA (EX. NEAR EA...|  31056997|        647500|                      48,0|                        0,00|        23,06|                            163,07|               700|        36,0|              3,2|     12,13|     0,22|    87,65|      1|     46,6|    20,34|       0,38|    0,24|   0,38|\n",
      "|    Albania |EASTERN EUROPE   ...|   3581655|         28748|                     124,6|                        1,26|        -4,93|                             21,52|              4500|        86,5|             71,2|     21,09|     4,42|    74,49|      3|    15,11|     5,22|      0,232|   0,188|  0,579|\n",
      "|    Algeria |NORTHERN AFRICA  ...|  32930091|       2381740|                      13,8|                        0,04|        -0,39|                                31|              6000|        70,0|             78,1|      3,22|     0,25|    96,53|      1|    17,14|     4,61|      0,101|     0,6|  0,298|\n",
      "+------------+--------------------+----------+--------------+--------------------------+----------------------------+-------------+----------------------------------+------------------+------------+-----------------+----------+---------+---------+-------+---------+---------+-----------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть схему файла можно с помощью метода `printSchema()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- Area (sq. mi.): integer (nullable = true)\n",
      " |-- Pop. Density (per sq. mi.): string (nullable = true)\n",
      " |-- Coastline (coast/area ratio): string (nullable = true)\n",
      " |-- Net migration: string (nullable = true)\n",
      " |-- Infant mortality (per 1000 births): string (nullable = true)\n",
      " |-- GDP ($ per capita): integer (nullable = true)\n",
      " |-- Literacy (%): string (nullable = true)\n",
      " |-- Phones (per 1000): string (nullable = true)\n",
      " |-- Arable (%): string (nullable = true)\n",
      " |-- Crops (%): string (nullable = true)\n",
      " |-- Other (%): string (nullable = true)\n",
      " |-- Climate: string (nullable = true)\n",
      " |-- Birthrate: string (nullable = true)\n",
      " |-- Deathrate: string (nullable = true)\n",
      " |-- Agriculture: string (nullable = true)\n",
      " |-- Industry: string (nullable = true)\n",
      " |-- Service: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка JSON файла (столицы) происходит полностью аналогично, меняются только формат файла и, возможно, набор опций (зависит от специфики файла)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----+----------+----------+------+-------+------+---+------------+---------+------+--------+----------+---------+----+--------+----------+-----+--------+-----------+-----+------+---------+----------+--------+--------+-------------------+-----+---+--------+------+-------+---+--------+-----+--------+------+-----------+--------+------+-----------+-----+------------+------+--------+-------+-------+------+--------+------+-----+-----------+----------------+-------+------+------+--------+----------+------+-------------+-------+-----+-------+-----+--------+------+------+-----------+--------+----+-------+-------+--------+-----+----------+------+------------+-------+-------+-------------+-----+---------+----+------+-------+-----------+------+------+---------+--------------+-------+------+----------+---------+---+-----------+------+--------------+--------+-------+------+---------+--------------------+---------+------------+-------+------+---------+----+------------+--------+-----+-----+-------+-------+----------+------+------+----------+---------+-----+-----------+-----------+------+---------+------+--------+-----+-------+--------+------+-------+----------+----+--------+-----+------+--------+---------+-------+------------+------+------+------+-----------+----------+-----+------+--------------+----------+--------+--------+----------+----+--------+-----------+------------+------+--------+------+------+--------+-----+-------+---------+----+---------+-----+-----+----------+------+-----------+----+-------+------------+------+---------+------+------------+---------+--------+--------------+------+--------+--------+----+-----------+---------+--------+------+------+------+-------+--------+--------+---------+--------+---------+---------+------------+----------+--------+----------+-----+---------+----------+----+--------+------------+-----------+--------+-------+-------------+---------+-----------------+----+-------+--------+---+----+--------+-----+----------+------+-------------+--------+------+------+----+-------+---+----------+----------+--------+------------+---------+-------+---------+----------------+-----+---------+--------+----+--------+-----+---------+--------+------+------+\n",
      "|              AD|       AE|   AF|        AG|        AI|    AL|     AM|    AO| AQ|          AR|       AS|    AT|      AU|        AW|       AX|  AZ|      BA|        BB|   BD|      BE|         BF|   BG|    BH|       BI|        BJ|      BL|      BM|                 BN|   BO| BQ|      BR|    BS|     BT| BV|      BW|   BY|      BZ|    CA|         CC|      CD|    CF|         CG|   CH|          CI|    CK|      CL|     CM|     CN|    CO|      CR|    CU|   CV|         CW|              CX|     CY|    CZ|    DE|      DJ|        DK|    DM|           DO|     DZ|   EC|     EE|   EG|      EH|    ER|    ES|         ET|      FI|  FJ|     FK|     FM|      FO|   FR|        GA|    GB|          GD|     GE|     GF|           GG|   GH|       GI|  GL|    GM|     GN|         GP|    GQ|    GR|       GS|            GT|     GU|    GW|        GY|       HK| HM|         HN|    HR|            HT|      HU|     ID|    IE|       IL|                  IM|       IN|          IO|     IQ|    IR|       IS|  IT|          JE|      JM|   JO|   JP|     KE|     KG|        KH|    KI|    KM|        KN|       KP|   KR|         KW|         KY|    KZ|       LA|    LB|      LC|   LI|     LK|      LR|    LS|     LT|        LU|  LV|      LY|   MA|    MC|      MD|       ME|     MF|          MG|    MH|    MK|    ML|         MM|        MN|   MO|    MP|            MQ|        MR|      MS|      MT|        MU|  MV|      MW|         MX|          MY|    MZ|      NA|    NC|    NE|      NF|   NG|     NI|       NL|  NO|       NP|   NR|   NU|        NZ|    OM|         PA|  PE|     PF|          PG|    PH|       PK|    PL|          PM|       PN|      PR|            PS|    PT|      PW|      PY|  QA|         RE|       RO|      RS|    RU|    RW|    SA|     SB|      SC|      SD|       SE|      SG|       SH|       SI|          SJ|        SK|      SL|        SM|   SN|       SO|        SR|  SS|      ST|          SV|         SX|      SY|     SZ|           TC|       TD|               TF|  TG|     TH|      TJ| TK|  TL|      TM|   TN|        TO|    TR|           TT|      TV|    TW|    TZ|  UA|     UG| UM|        US|        UY|      UZ|          VA|       VC|     VE|       VG|              VI|   VN|       VU|      WF|  WS|      XK|   YE|       YT|      ZA|    ZM|    ZW|\n",
      "+----------------+---------+-----+----------+----------+------+-------+------+---+------------+---------+------+--------+----------+---------+----+--------+----------+-----+--------+-----------+-----+------+---------+----------+--------+--------+-------------------+-----+---+--------+------+-------+---+--------+-----+--------+------+-----------+--------+------+-----------+-----+------------+------+--------+-------+-------+------+--------+------+-----+-----------+----------------+-------+------+------+--------+----------+------+-------------+-------+-----+-------+-----+--------+------+------+-----------+--------+----+-------+-------+--------+-----+----------+------+------------+-------+-------+-------------+-----+---------+----+------+-------+-----------+------+------+---------+--------------+-------+------+----------+---------+---+-----------+------+--------------+--------+-------+------+---------+--------------------+---------+------------+-------+------+---------+----+------------+--------+-----+-----+-------+-------+----------+------+------+----------+---------+-----+-----------+-----------+------+---------+------+--------+-----+-------+--------+------+-------+----------+----+--------+-----+------+--------+---------+-------+------------+------+------+------+-----------+----------+-----+------+--------------+----------+--------+--------+----------+----+--------+-----------+------------+------+--------+------+------+--------+-----+-------+---------+----+---------+-----+-----+----------+------+-----------+----+-------+------------+------+---------+------+------------+---------+--------+--------------+------+--------+--------+----+-----------+---------+--------+------+------+------+-------+--------+--------+---------+--------+---------+---------+------------+----------+--------+----------+-----+---------+----------+----+--------+------------+-----------+--------+-------+-------------+---------+-----------------+----+-------+--------+---+----+--------+-----+----------+------+-------------+--------+------+------+----+-------+---+----------+----------+--------+------------+---------+-------+---------+----------------+-----+---------+--------+----+--------+-----+---------+--------+------+------+\n",
      "|Andorra la Vella|Abu Dhabi|Kabul|St. John's|The Valley|Tirana|Yerevan|Luanda|   |Buenos Aires|Pago Pago|Vienna|Canberra|Oranjestad|Mariehamn|Baku|Sarajevo|Bridgetown|Dhaka|Brussels|Ouagadougou|Sofia|Manama|Bujumbura|Porto-Novo|Gustavia|Hamilton|Bandar Seri Begawan|Sucre|   |Brasilia|Nassau|Thimphu|   |Gaborone|Minsk|Belmopan|Ottawa|West Island|Kinshasa|Bangui|Brazzaville|Berne|Yamoussoukro|Avarua|Santiago|Yaounde|Beijing|Bogota|San Jose|Havana|Praia| Willemstad|Flying Fish Cove|Nicosia|Prague|Berlin|Djibouti|Copenhagen|Roseau|Santo Domingo|Algiers|Quito|Tallinn|Cairo|El-Aaiun|Asmara|Madrid|Addis Ababa|Helsinki|Suva|Stanley|Palikir|Torshavn|Paris|Libreville|London|St. George's|Tbilisi|Cayenne|St Peter Port|Accra|Gibraltar|Nuuk|Banjul|Conakry|Basse-Terre|Malabo|Athens|Grytviken|Guatemala City|Hagatna|Bissau|Georgetown|Hong Kong|   |Tegucigalpa|Zagreb|Port-au-Prince|Budapest|Jakarta|Dublin|Jerusalem|Douglas, Isle of Man|New Delhi|Diego Garcia|Baghdad|Tehran|Reykjavik|Rome|Saint Helier|Kingston|Amman|Tokyo|Nairobi|Bishkek|Phnom Penh|Tarawa|Moroni|Basseterre|Pyongyang|Seoul|Kuwait City|George Town|Astana|Vientiane|Beirut|Castries|Vaduz|Colombo|Monrovia|Maseru|Vilnius|Luxembourg|Riga|Tripolis|Rabat|Monaco|Chisinau|Podgorica|Marigot|Antananarivo|Majuro|Skopje|Bamako|Nay Pyi Taw|Ulan Bator|Macao|Saipan|Fort-de-France|Nouakchott|Plymouth|Valletta|Port Louis|Male|Lilongwe|Mexico City|Kuala Lumpur|Maputo|Windhoek|Noumea|Niamey|Kingston|Abuja|Managua|Amsterdam|Oslo|Kathmandu|Yaren|Alofi|Wellington|Muscat|Panama City|Lima|Papeete|Port Moresby|Manila|Islamabad|Warsaw|Saint-Pierre|Adamstown|San Juan|East Jerusalem|Lisbon|Melekeok|Asuncion|Doha|Saint-Denis|Bucharest|Belgrade|Moscow|Kigali|Riyadh|Honiara|Victoria|Khartoum|Stockholm|Singapur|Jamestown|Ljubljana|Longyearbyen|Bratislava|Freetown|San Marino|Dakar|Mogadishu|Paramaribo|Juba|Sao Tome|San Salvador|Philipsburg|Damascus|Mbabane|Cockburn Town|N'Djamena|Port-aux-Francais|Lome|Bangkok|Dushanbe|   |Dili|Ashgabat|Tunis|Nuku'alofa|Ankara|Port of Spain|Funafuti|Taipei|Dodoma|Kiev|Kampala|   |Washington|Montevideo|Tashkent|Vatican City|Kingstown|Caracas|Road Town|Charlotte Amalie|Hanoi|Port Vila|Mata Utu|Apia|Pristina|Sanaa|Mamoudzou|Pretoria|Lusaka|Harare|\n",
      "+----------------+---------+-----+----------+----------+------+-------+------+---+------------+---------+------+--------+----------+---------+----+--------+----------+-----+--------+-----------+-----+------+---------+----------+--------+--------+-------------------+-----+---+--------+------+-------+---+--------+-----+--------+------+-----------+--------+------+-----------+-----+------------+------+--------+-------+-------+------+--------+------+-----+-----------+----------------+-------+------+------+--------+----------+------+-------------+-------+-----+-------+-----+--------+------+------+-----------+--------+----+-------+-------+--------+-----+----------+------+------------+-------+-------+-------------+-----+---------+----+------+-------+-----------+------+------+---------+--------------+-------+------+----------+---------+---+-----------+------+--------------+--------+-------+------+---------+--------------------+---------+------------+-------+------+---------+----+------------+--------+-----+-----+-------+-------+----------+------+------+----------+---------+-----+-----------+-----------+------+---------+------+--------+-----+-------+--------+------+-------+----------+----+--------+-----+------+--------+---------+-------+------------+------+------+------+-----------+----------+-----+------+--------------+----------+--------+--------+----------+----+--------+-----------+------------+------+--------+------+------+--------+-----+-------+---------+----+---------+-----+-----+----------+------+-----------+----+-------+------------+------+---------+------+------------+---------+--------+--------------+------+--------+--------+----+-----------+---------+--------+------+------+------+-------+--------+--------+---------+--------+---------+---------+------------+----------+--------+----------+-----+---------+----------+----+--------+------------+-----------+--------+-------+-------------+---------+-----------------+----+-------+--------+---+----+--------+-----+----------+------+-------------+--------+------+------+----+-------+---+----------+----------+--------+------------+---------+-------+---------+----------------+-----+---------+--------+----+--------+-----+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# загрузим JSON\n",
    "dfj = spark.read.format(\"json\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"path\", \"data/capital.json\") \\\n",
    "    .load()\n",
    "dfj.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запись в файлы происходит так же просто (и универсально - нужно лишь указать формат результирующего файла)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# цепочка обработки при записи\n",
    "df.write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .save(\"data/new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним в виде JSON\n",
    "df.write.format(\"json\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"data/new.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Комментарии\n",
    "\n",
    "В-целом работать с файлами в Spark проще, чем без него (вспомним наш способ копирования файлов через внешние таблицы - там было гораздо больше действий). Общим действием является начало процесса - копирование файла в HDFS (см. ниже).\n",
    "\n",
    "Важные моменты:\n",
    "\n",
    "* при чтении файл должен быть в HDFS (для кластерного варианта), имя - директория\n",
    "* при записи файл окажется в HDFS, имя - директория, заставить записать единичный файл - сложно...\n",
    "* при записи есть проблема с заголовками (например, в CSV файлах) - они либо будут отсутствовать во всех файлах, либо присутствовать (что неудобно)\n",
    "* mode - опция при чтении, метод - при записи...\n",
    "* запятая в числах - фатально, преобразовывать до или после\n",
    "* параллелизм не посмотреть - см. материалы\n",
    "* опций много и они разные для разных видов файлов, см. материалы про основные из них\n",
    "* при чтении можно указывать схему (`.schema(someSchema)`)\n",
    "\n",
    "### Поддерживаются файлы\n",
    "\n",
    "* CSV\n",
    "* JSON\n",
    "* Parquet\n",
    "* ORC\n",
    "* текстовые\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**скринкаст 2**\n",
    "\n",
    "### Работа с базами данных\n",
    "\n",
    "Для работы с базой данных нам нужна\n",
    "\n",
    "* база данных (поэтому выполняем в кластере)\n",
    "* JDBC драйвера\n",
    "\n",
    "особенности установки драйверов - см. материалы предыдущего шага.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С базами данных будем работать из кластера (в `cluster mode`), шапка стандартная, но чуть другая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настраиваем окружение для работы spark\n",
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "#os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--executor-memory 2G --num-executors 16 --executor-cores 2 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"\"\"--driver-class-path /usr/share/java/postgresql.jar \n",
    "--jars /usr/share/java/postgresql.jar --executor-memory 600M --driver-memory 1G --num-executors 1 pyspark-shell\"\"\"\n",
    "\n",
    "# добавляем модуль в путь\n",
    "import sys\n",
    "sys.path.append('/opt/cloudera/parcels/CDH-6.3.1-1.cdh6.3.1.p0.1470567/lib/spark/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "master = \"yarn\"\n",
    "spark = SparkSession.builder.master(master).appName(\"spark_test\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение из базы данных с помощью Spark - это просто, указывается абсолютный минимум параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# цепочка обработки при чтении\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\",\"jdbc:postgresql://localhost:7432/scm\") \\\n",
    "    .option(\"dbtable\",\"hosts\") \\\n",
    "    .option(\"user\",\"cloudera-scm\") \\\n",
    "    .option(\"password\",\"2MhalIGcSp\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запись в базу данных (Hive) происходит так же просто (как и запись в файл), фактически меняется один метод - вместо `save()` вызываем `saveAsTable()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# цепочка обработки при записи\n",
    "jdbcDF.write.format(\"parquet\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .option(\"compression\",\"gzip\") \\\n",
    "    .saveAsTable(\"sp_hosts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Комментарии\n",
    "\n",
    "* см. предыдущий шаг про настройку JDBC\n",
    "* вместо имени таблицы в опции `dbtable` может стоять `select` (см. документацию)\n",
    "* схема данных переносится автоматически\n",
    "* сохранять можно не только в Hive, в любую реляционную СУБД (для которой настроен JDBC доступ)\n",
    "* обратите внимание на `enableHiveSupport()` при создании сессии - это нужно для того, чтобы Spark работал с теми же метаданными, что и Hive\n",
    "* мы познакомимся с еще одним способом загрузки данных из реляционных СУБД, когда будем говорить про Spark SQL\n",
    "* параллелизм посмотреть на таком кластере не получится, см. материалы ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эффективная работа с базами данных и файлами (параллелизм)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для эффективной (параллельной) работы с базами данных и файлами нужно\n",
    "\n",
    "* наличие нескольких executor-ов\n",
    "* разбиение dataframe на разделы (partitions)\n",
    "\n",
    "**Executor-ы**\n",
    "\n",
    "Их количество мы указываем в параметрах запуска Spark приложения (см. предыщущий шаг).\n",
    "\n",
    "**Разделы**\n",
    "\n",
    "Для файлов разделы \"наследуются\" из способа разбиения файла на split-ы (см. HDFS).\n",
    "\n",
    "Для реляционных баз данных мы задаем количество партиций явно (см. параметр `numPartitions` ниже).\n",
    "\n",
    "Кроме того, необходимо (как и в случае `sqoop`) \"подсказать\" Spark-у - как правильнее разбить исходную таблицу на разделы (правильнее = равномернее). Для этого служит набор опций, перечисленный ниже. Идеология ровно такая же, как и в случае `sqoop` - нужно указать примерный диапазон значений числовой колонки, по которой производится разбиение, само разделение обеспечит Spakr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    .option(\"lowerBound\", 712415) \\\n",
    "    .option(\"upperBound\", 81792182) \\\n",
    "    .option(\"partitionColumn\", \"CONTRACT_ID\") \\\n",
    "    .option(\"numPartitions\", 100) \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При записи количество разделов будет совпадать с количеством файлов в директории с таблицей, с помощью вызова метода `repartition()` можно явно управлять этим количеством (вызов метода приведет к так называемому `shuffle` - передаче данных между узлами кластера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема плохих полей в файлах**\n",
    "\n",
    "Если при загрузке данных из файлов (например, CSV) в файле в полях присутствуют разделители (полей или строк), то загрузка закончится плохо (также, как и при работе с внешними файлами). Чтобы бороться с этим в Spark есть опции, позволяющие задать разделители полей и строк, а также позволяющие загружать \"экранированные\" кавычками строки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопросы для самопроверки**\n",
    "\n",
    "* для каких файлов нужна явная схема\n",
    "* как будут записаны файлы (директория)\n",
    "* как задать присутствие заголовка в csv\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
